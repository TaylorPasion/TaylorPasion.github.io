---
layout: default
title: "Week 3"
permalink: /week3/
---
## ðŸ§  Transformer Model Text Analysis

We implemented a transformer-based large language model (LLM) to analyze text and conversational data.  
This project provided hands-on experience with the architecture and inner workings of transformer models,  
while deepening our understanding of how LLMs process, interpret, and generate natural language.

**The goal:**  
To explore how transformer models operate and to gain practical experience in building, training, and applying an LLM to real-world text inputs.

---

### ðŸ”— Attention Mechanism

The image below shows an attention map, which visualizes how correlated each word is to the others in a sentence.  
This is a core mechanism in transformer modelsâ€”it allows the model to weigh the importance of each word relative to every other word.

<p align="center">
  <img src="https://github.com/user-attachments/assets/676a431e-04d7-46cd-abc2-f9a86a624e86" alt="Attention Map" width="600">
</p>

---

### ðŸ’¬ Conversation Summarization

Below is an example of how the model was trained using real conversations.  
It shows how the transformer learns to summarize long text inputs into concise outputs.

<p align="center">
  <img src="https://github.com/user-attachments/assets/b7726e92-bbf8-4500-b049-e0c1c8fbbf41" alt="Conversation Summarization Training" width="900">
</p>
